version: "3.8"
services:
  zookeeper:
    image: wurstmeister/zookeeper
    deploy:
      resources:
        limits:
          memory: 64G    

  kafka:
    image: wurstmeister/kafka
    deploy:
      resources:
        limits:
          memory: 8G    
   
    ports:
      - "${KAFKA_PORT}:9092"
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      DOCKER_API_VERSION: 1.22
      HOSTNAME_COMMAND: "route -n | awk '/UG[ \t]/{print $$2}'"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "test_0:1:1:delete,test_1:1:1:delete"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: -1
      KAFKA_LOG_RETENTION_BYTES: -1
      KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS: 1000000000
      KAFKA_LOG_SEGMENT_MS: 10000
      KAFKA_MIN_CLEANABLE_DIRTY_RATIO: 1
    healthcheck:
      test: [ "CMD", "nc", "-vz", "localhost", "9092" ]
      interval: 10s
      timeout: 10s
      retries: 10
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
  spark-master:  
    build: ../services/spark-cluster/
    deploy:
      resources:
        limits:
          memory: 16G    
 
    depends_on:
      - kafka
    ports:
      - "${SPARK_MASTER_UI_PORT}:8080"
      - "${SPARK_MASTER_PORT}:7077"
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
  
  spark-worker:    
    build: ../services/spark-cluster/
    deploy:
      replicas: ${WORKERS}
      resources:
        limits:
          memory: 16G    

    ports:
      - "9000-9010:8080"
      - "7000-7010:7000"
    depends_on:
      - spark-master
      
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=16G
      - SPARK_DRIVER_MEMORY=16G
      - SPARK_EXECUTOR_MEMORY=16G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker

  spark-word-count:
    deploy:
      resources:
        limits:
          memory: 16G    
   
    build: ../services/spark-word-count-scala/
    depends_on:
      - spark-master
      - spark-worker
    environment:
      - AUTOCOMMIT_FREQUENCY_MS
      - BENCHMARK_TYPE
      - COMMIT_FREQUENCY
      - RATE_PER_SECOND
      - CORES
      - WORKERS
         
  streamer:  
    build: ../services/streamer/
    deploy:
      resources:
        limits:
          memory: 16G    
  
    environment:
      - AUTOCOMMIT_FREQUENCY_MS
      - BENCHMARK_TYPE
      - COMMIT_FREQUENCY
      - RATE_PER_SECOND
    depends_on:
      kafka:
        condition: service_healthy
      spark-word-count:
        condition: service_started
  stats-collector:
    build: ../services/stats-collector/
    deploy:
      resources:
        limits:
          memory: 16G    
   
    environment:
      - AUTOCOMMIT_FREQUENCY_MS
      - BENCHMARK_TYPE
      - COMMIT_FREQUENCY
      - RATE_PER_SECOND
      - ENGINE_TYPE
      - CORES
      - WORKERS
    depends_on:
      streamer:
        condition: service_completed_successfully
    volumes:
      - "./results:/stats-collector/results"
